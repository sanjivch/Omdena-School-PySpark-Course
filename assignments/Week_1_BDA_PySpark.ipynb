{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjivch/Omdena-School-PySpark-Course/blob/main/assignments/Week_1_BDA_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "423415a4-057e-40a5-a4d3-273ff0f2da2c",
      "metadata": {
        "id": "423415a4-057e-40a5-a4d3-273ff0f2da2c"
      },
      "source": [
        "# `BIG DATA FUNDAMENTALS WITH PYSPARK`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abe42a9a-b7fd-4b8a-8ce5-fc0584504f65",
      "metadata": {
        "id": "abe42a9a-b7fd-4b8a-8ce5-fc0584504f65"
      },
      "source": [
        "## What is Big Data?\n",
        "Big data is a term used to refer to the study and applications of data sets that are too\n",
        "complex for traditional data-processing software `- wikipedia`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "757f61ed-b7da-4b8a-b183-b23357d31dd9",
      "metadata": {
        "id": "757f61ed-b7da-4b8a-b183-b23357d31dd9"
      },
      "source": [
        "## The 3 V's of Big Data\n",
        "- `Volume:` Variety and Velocity\n",
        "- `Volume:` Size of the data\n",
        "- `Variety:` Different sources & formats\n",
        "- `Velocity:` Speed of teh data "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9265051a-f468-477a-8999-06da73f799ee",
      "metadata": {
        "id": "9265051a-f468-477a-8999-06da73f799ee"
      },
      "source": [
        "## Big Data concepts and Terminology\n",
        "- `Clustered computing:` Collection of resources of multiple machines\n",
        "- `Parallel computing:` Simultaneous computation\n",
        "- `Distributed computing:` Collection of nodes (networked computers) that run in parallel\n",
        "- `Batch processing:` Breaking the job into small pieces and running them on individual\n",
        "machines\n",
        "- `Real-time processing:` Immediate processing of data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fff3ff5-cdef-4b46-a5a1-93f163bf5f76",
      "metadata": {
        "id": "6fff3ff5-cdef-4b46-a5a1-93f163bf5f76"
      },
      "source": [
        "NOTES: **Clustered computing** is the pooling of resources of multiple machines to complete jobs. **Parallel computing** is a type of computation in which many calculations are carried out simultaneously. A **distributed computing** involves nodes or networked computers that run jobs in parallel. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a73955-9f17-4d52-b22e-37ec37e10d58",
      "metadata": {
        "id": "02a73955-9f17-4d52-b22e-37ec37e10d58"
      },
      "source": [
        "## Big Data processing systems\n",
        "- `Hadoop/MapReduce:` Scalable and fault tolerant framework written in Java\n",
        "    - Open SOurce\n",
        "    - Batch Processing\n",
        "- `Apache Spark:` General purpose and lightning fast cluster computing system\n",
        "    - Open source\n",
        "    - Both batch and real-time data processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa726cb-4ff1-41a9-9053-ee7fc30ba910",
      "metadata": {
        "id": "caa726cb-4ff1-41a9-9053-ee7fc30ba910"
      },
      "source": [
        "## Features of Apache Spark framework\n",
        "- Distributed cluster computing framework\n",
        "- Efficient in-memory computations for large data sets\n",
        "- Lightning fast data processing framework\n",
        "- Provides support for Java, Scala, Python, R and SQ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db586398-1542-46e7-af82-b727bdc180c7",
      "metadata": {
        "id": "db586398-1542-46e7-af82-b727bdc180c7"
      },
      "source": [
        "## Spark modes of deployment\n",
        "- `Local mode:` Single machine such as your laptop\n",
        "    - Local model convenient for testing, debugging and demonstration\n",
        "- `Cluster mode:` Set of pre-defined machines\n",
        "    - Good for production\n",
        "- Workfkow: Local -> Cluster\n",
        "- No Code Changes Necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569d2d43-41e9-475a-a130-f8f69c554ea5",
      "metadata": {
        "id": "569d2d43-41e9-475a-a130-f8f69c554ea5"
      },
      "source": [
        "# `PySpark: Spark with Python`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bc3eb80-5b25-444d-a713-f91fb7b30ff3",
      "metadata": {
        "id": "0bc3eb80-5b25-444d-a713-f91fb7b30ff3"
      },
      "source": [
        "## Overview of PySpark\n",
        "- Apache Spark is written in Scala\n",
        "- To support Python with Spark, Apache Spark Community released PySpark\n",
        "- Similar computation speed and power as Scala\n",
        "- PySpark APIs are similar to Pandas and Scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91233950-9b0f-4f9f-be82-4a505aef723a",
      "metadata": {
        "id": "91233950-9b0f-4f9f-be82-4a505aef723a"
      },
      "source": [
        "## What is Spark shell?\n",
        "- Interactive environment for running Spark jobs\n",
        "- Helpful for fast interactive prototyping\n",
        "- Spark’s shells allow interacting with data on disk or in memory\n",
        "- Three different Spark shells:\n",
        "    - Spark-shell for Scala\n",
        "    - PySpark-shell for Python\n",
        "    - SparkR for R"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3761d39a-1c6b-4ecc-9193-2823fa578253",
      "metadata": {
        "id": "3761d39a-1c6b-4ecc-9193-2823fa578253"
      },
      "source": [
        "## PySpark shell\n",
        "- PySpark shell is the Python-based command line tool\n",
        "- PySpark shell allows data scientists interface with Spark data structures\n",
        "- PySpark shell support connecting to a cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b222ed77-60f7-4b30-b0df-95f54db5c873",
      "metadata": {
        "id": "b222ed77-60f7-4b30-b0df-95f54db5c873"
      },
      "source": [
        "## Understanding `SparkContext`\n",
        "- `SparkContext` is an entry point into the world of Spark\n",
        "- An entry point is a way of connecting to Spark cluster\n",
        "- An entry point is like a key to the house\n",
        "- PySpark has a default `SparkContext` called **sc**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf02b85-763b-463b-88f7-63542b810601",
      "metadata": {
        "id": "ecf02b85-763b-463b-88f7-63542b810601"
      },
      "source": [
        "NOTES:\n",
        "- An **entry point** is where control is transferred from the Operating system to the provided program. \n",
        "- In simpler terms, it's like a key to your house. Without the key you cannot enter the house, similarly, without an entry point, you cannot run any PySpark jobs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88341c11-a5be-4b7c-b516-d7aa64e789ad",
      "metadata": {
        "id": "88341c11-a5be-4b7c-b516-d7aa64e789ad"
      },
      "source": [
        "### Inspecting SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ccf01ad6-dcd3-40eb-a890-b37199781d2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccf01ad6-dcd3-40eb-a890-b37199781d2d",
        "outputId": "fdc32a63-8b18-4e3a-c82e-61a5318e5840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 37 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 51.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=d6a6cf1601e95311dfdfdc7b6da3c3c7df944ea1fc9ba99bab423917ca09cd7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        }
      ],
      "source": [
        "#Verifying Spark \n",
        "!pip install pyspark "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8aafbf3b-0e58-4576-aa0f-792845f58b23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aafbf3b-0e58-4576-aa0f-792845f58b23",
        "outputId": "6c9f917d-d3b3-460b-d6c9-662a2090ca1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "# finding Pyspark \n",
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "46171c1f-36d4-425b-afed-b75193df4ef9",
      "metadata": {
        "id": "46171c1f-36d4-425b-afed-b75193df4ef9"
      },
      "outputs": [],
      "source": [
        "#importing Libraries\n",
        "from pyspark import SparkConf\n",
        "from pyspark.context import SparkContext\n",
        "\n",
        "#creating SparkContext\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed959090-8fc8-4466-a457-c987ffbcbe7c",
      "metadata": {
        "id": "ed959090-8fc8-4466-a457-c987ffbcbe7c"
      },
      "source": [
        "`Version:` To retrieve SparkContext version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2a0aa46-5cdd-4c6b-b81d-170df90d27d2",
      "metadata": {
        "id": "a2a0aa46-5cdd-4c6b-b81d-170df90d27d2",
        "outputId": "1ff24a2c-5380-4e4c-efd6-936daaf3c13d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.1.3'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sc.version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c4e166-8cbc-489b-bd0e-2130ffbab886",
      "metadata": {
        "id": "83c4e166-8cbc-489b-bd0e-2130ffbab886"
      },
      "source": [
        "`Python Version:` To retrieve Python version of SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "520d0960-72ae-4095-acf2-aabdcea6e585",
      "metadata": {
        "id": "520d0960-72ae-4095-acf2-aabdcea6e585",
        "outputId": "3d40d4ee-5b19-4311-b092-cecc860e7178"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'3.9'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sc.pythonVer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9da0a9a-0ad7-48fc-a9c5-6df2940f46ed",
      "metadata": {
        "id": "b9da0a9a-0ad7-48fc-a9c5-6df2940f46ed"
      },
      "source": [
        "`Master:` URL of the cluster or “local” string to run in local mode of SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4435e02f-a97b-4108-bb0d-43de824f1c0f",
      "metadata": {
        "id": "4435e02f-a97b-4108-bb0d-43de824f1c0f",
        "outputId": "c31b4add-cdf2-4cff-aff2-72ccfbb4420d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'local[*]'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sc.master"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b06f85a-94eb-4b25-bfae-bc85defa2a13",
      "metadata": {
        "tags": [],
        "id": "6b06f85a-94eb-4b25-bfae-bc85defa2a13"
      },
      "source": [
        "### Loading data in PySpark\n",
        "\n",
        "- SparkContext's `parallelize()` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a91ab0b-0c39-4095-a890-51073e5357d2",
      "metadata": {
        "id": "1a91ab0b-0c39-4095-a890-51073e5357d2"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize([1,2,3,4,5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a06f786-ef6c-49ee-b5a5-f49f4706ab47",
      "metadata": {
        "id": "5a06f786-ef6c-49ee-b5a5-f49f4706ab47"
      },
      "source": [
        "- SparkContext's `textFile()` method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b49ae5d-76b0-4b0d-a3d9-21fd5f4286c5",
      "metadata": {
        "id": "1b49ae5d-76b0-4b0d-a3d9-21fd5f4286c5"
      },
      "outputs": [],
      "source": [
        "rdd2 = sc.textFile(\"test.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10cd9250-0631-4ede-8f16-f20a631cfcf7",
      "metadata": {
        "id": "10cd9250-0631-4ede-8f16-f20a631cfcf7"
      },
      "source": [
        "## Use of function in python - `lambda()`, `map()`, `filter()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52cbcfcb-e5a3-476d-a84a-8e77746e7568",
      "metadata": {
        "id": "52cbcfcb-e5a3-476d-a84a-8e77746e7568"
      },
      "source": [
        "### What are anonymous functions in Python?\n",
        "- Lambda functions are anonymous functions in Python\n",
        "- Very powerful and used in Python. Quite effiencetly with `map()` and `filter()`\n",
        "- Lambda functions create functions to be called later similar to def\n",
        "- It returns the functions without any name (i.e anonymous)\n",
        "- Inline a function definition or to defer execution of a code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "552d06f0-cd4f-4ce8-8382-fd13c4ff1b3d",
      "metadata": {
        "id": "552d06f0-cd4f-4ce8-8382-fd13c4ff1b3d"
      },
      "source": [
        "### Lambda function syntax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d722974-f44b-4f7b-b022-d234f8145c01",
      "metadata": {
        "id": "0d722974-f44b-4f7b-b022-d234f8145c01"
      },
      "source": [
        "- The general form of lambda functions is\n",
        "\n",
        "`>>> lambda arguments: expression`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "103960cf-c2e1-4958-bcb0-f69a8b7fcc91",
      "metadata": {
        "id": "103960cf-c2e1-4958-bcb0-f69a8b7fcc91"
      },
      "source": [
        "- Example of lambda function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371b1e8e-e015-4345-97b8-d091cc1db99a",
      "metadata": {
        "id": "371b1e8e-e015-4345-97b8-d091cc1db99a",
        "outputId": "49c436d6-72b2-42db-c529-f3484fe3b504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "double = lambda x: x * 2\n",
        "print(double(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5517ced9-3a13-4ca3-b1b8-41bd4dbd0ae5",
      "metadata": {
        "id": "5517ced9-3a13-4ca3-b1b8-41bd4dbd0ae5"
      },
      "source": [
        "### Difference between def vs lambda functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7ca2cc2-4152-44e3-9afc-ec567d7e4d8e",
      "metadata": {
        "id": "f7ca2cc2-4152-44e3-9afc-ec567d7e4d8e"
      },
      "source": [
        "- Python code to illustrate cube of a number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "664432b0-bdaf-4b45-885f-6a9ec57b0f6c",
      "metadata": {
        "id": "664432b0-bdaf-4b45-885f-6a9ec57b0f6c",
        "outputId": "29fc9899-17c1-464b-c8fd-cab748c98f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "#python Funtion\n",
        "def cube(x):\n",
        "    return x ** 3\n",
        "\n",
        "#lambda function\n",
        "g = lambda x: x ** 3\n",
        "\n",
        "#displaying on console\n",
        "print(g(10))\n",
        "print(cube(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4391f11c-cf81-446e-9ee6-f9ed097f2ba9",
      "metadata": {
        "id": "4391f11c-cf81-446e-9ee6-f9ed097f2ba9"
      },
      "source": [
        "### conclusion\n",
        "- No return statement for lambda\n",
        "- Can put lambda function anywhere\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ccdbcb1-bc8a-4458-9941-65810fc48bcf",
      "metadata": {
        "id": "9ccdbcb1-bc8a-4458-9941-65810fc48bcf"
      },
      "source": [
        "### Use of Lambda function in Python - `map()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a74c35-e795-4e0a-a7f4-a7faaf613cf2",
      "metadata": {
        "id": "58a74c35-e795-4e0a-a7f4-a7faaf613cf2"
      },
      "source": [
        "- `map()` function takes a function and a list and returns a new list which contains items\n",
        "returned by that function for each item\n",
        "- General syntax of `map()`\n",
        "\n",
        "`>>> map(function, list)`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5943baa4-4103-437a-8484-b8941bc94c97",
      "metadata": {
        "id": "5943baa4-4103-437a-8484-b8941bc94c97"
      },
      "source": [
        "- Example of `map()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbbb7d7e-0ec8-43f2-999b-84b81737fb7f",
      "metadata": {
        "id": "dbbb7d7e-0ec8-43f2-999b-84b81737fb7f",
        "outputId": "00f9e735-e0bb-450f-8765-03f178ab4c48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[3, 4, 5, 6]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "items = [1, 2, 3, 4]\n",
        "list(map(lambda x: x + 2 , items))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bac2700-ec34-44c7-abf3-06cbd56bf447",
      "metadata": {
        "id": "8bac2700-ec34-44c7-abf3-06cbd56bf447"
      },
      "source": [
        "### Use of Lambda function in python - `filter()`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ed2e62-dabb-4680-9dde-7d4a63253b49",
      "metadata": {
        "id": "04ed2e62-dabb-4680-9dde-7d4a63253b49"
      },
      "source": [
        "- `filter()` function takes a function and a list  and returns  a new list for which the function evaluates as `True`\n",
        "- General Syntax of `filter()`\n",
        "\n",
        "`>>> filter(function, list)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60d175a6-6aa7-48c6-beac-c3d3021ad1d6",
      "metadata": {
        "id": "60d175a6-6aa7-48c6-beac-c3d3021ad1d6"
      },
      "source": [
        "- Example of `filter()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e8c15ef-4e5b-4ef8-b652-55fec50226f0",
      "metadata": {
        "id": "0e8c15ef-4e5b-4ef8-b652-55fec50226f0",
        "outputId": "abdaee31-8961-4e29-f565-7c6a8862a7a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 3]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "items = [1, 2, 3, 4]\n",
        "list(filter(lambda x: (x%2 != 0), items))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ed84e9a-85c2-471a-8ff1-e7579a58db2c",
      "metadata": {
        "id": "7ed84e9a-85c2-471a-8ff1-e7579a58db2c"
      },
      "source": [
        "# EXERCISE:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f27465-7590-4974-80f7-30073b94c742",
      "metadata": {
        "id": "56f27465-7590-4974-80f7-30073b94c742"
      },
      "source": [
        "- Print the version of SparkContext in the PySpark shell.\n",
        "- Print the Python version of SparkContext in the PySpark shell.\n",
        "- What is the master of SparkContext in the PySpark shell?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c362f8af-8c16-4f2d-b3f3-4030409337ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c362f8af-8c16-4f2d-b3f3-4030409337ee",
        "outputId": "8eb529d3-7519-4cad-ab87-5bd9d6ca3ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The version of Spark Context in the PySpark shell is 3.3.0\n",
            "The Python version of Spark Context in the PySpark shell is 3.7\n",
            "The master of Spark Context in the PySpark shell is local[*]\n"
          ]
        }
      ],
      "source": [
        "# Print the version of SparkContext\n",
        "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
        "\n",
        "# Print the Python version of SparkContext\n",
        "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
        "\n",
        "# Print the master of SparkContext\n",
        "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7adc4b29-7ac4-4e97-8c85-a00d749303c6",
      "metadata": {
        "id": "7adc4b29-7ac4-4e97-8c85-a00d749303c6"
      },
      "source": [
        "- Create a Python list named numb containing the numbers 1 to 100.\n",
        "- Load the list into Spark using Spark Context's parallelize method and assign it to a variable spark_data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eb6fabaf-7d6b-4275-afc3-d20ca442d557",
      "metadata": {
        "id": "eb6fabaf-7d6b-4275-afc3-d20ca442d557"
      },
      "outputs": [],
      "source": [
        "# Create a Python list of numbers from 1 to 100 \n",
        "numb = range(1, 101)\n",
        "\n",
        "# Load the list into PySpark  \n",
        "spark_data = sc.parallelize(numb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60da9f1b-9287-49d3-a580-f12ac489f3ca",
      "metadata": {
        "id": "60da9f1b-9287-49d3-a580-f12ac489f3ca"
      },
      "source": [
        "- Load a local text file README.md in PySpark shell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0b9df3a7-4560-403d-822b-fc2fffa37c4c",
      "metadata": {
        "id": "0b9df3a7-4560-403d-822b-fc2fffa37c4c"
      },
      "outputs": [],
      "source": [
        "# Load a local file into PySpark shell\n",
        "lines = sc.textFile(\"test.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05d5d9c2-e2ae-4834-a5a4-6023e623d533",
      "metadata": {
        "id": "05d5d9c2-e2ae-4834-a5a4-6023e623d533"
      },
      "source": [
        "- Print my_list which is available in your environment.\n",
        "- Square each item in my_list using map() and lambda().\n",
        "- Print the result of map function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d31b8530-5a90-4510-95d5-063b0c41c5a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d31b8530-5a90-4510-95d5-063b0c41c5a9",
        "outputId": "49775ab3-6b35-48f3-8189-f2d214f1a3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input list is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "The squared numbers are [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
          ]
        }
      ],
      "source": [
        "# list contaning 1 to 10 numbers\n",
        "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "# Print my_list in the console\n",
        "print(\"Input list is\", my_list)\n",
        "\n",
        "# Square all numbers in my_list\n",
        "squared_list_lambda = list(map(lambda x: x**2, my_list))\n",
        "\n",
        "# Print the result of the map function\n",
        "print(\"The squared numbers are\", squared_list_lambda)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d99ddf05-666e-4e87-8872-c6e016e0291e",
      "metadata": {
        "id": "d99ddf05-666e-4e87-8872-c6e016e0291e"
      },
      "source": [
        "- Print my_list2 which is available in your environment.\n",
        "- Filter the numbers divisible by 10 from my_list2 using filter() and lambda().\n",
        "- Print the numbers divisible by 10 from my_list2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "816a8294-98a0-410b-9a18-6b58bde0f9f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "816a8294-98a0-410b-9a18-6b58bde0f9f0",
        "outputId": "77f574e0-d9bf-4c1e-c08a-b7e645ff89dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input list is: [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
            "Numbers divisible by 10 are: [10, 40, 60, 80]\n"
          ]
        }
      ],
      "source": [
        "# mylist2 defined \n",
        "my_list2 = [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\n",
        "\n",
        "# Print my_list2 in the console\n",
        "print(\"Input list is:\", my_list2)\n",
        "\n",
        "# Filter numbers divisible by 10\n",
        "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\n",
        "\n",
        "# Print the numbers divisible by 10\n",
        "print(\"Numbers divisible by 10 are:\", filtered_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b86d5a0-9de9-434a-ac5c-124a83389983",
      "metadata": {
        "id": "0b86d5a0-9de9-434a-ac5c-124a83389983"
      },
      "source": [
        "## Good Luck :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c442282-50af-4265-9ff2-5c74f9b36dff",
      "metadata": {
        "id": "4c442282-50af-4265-9ff2-5c74f9b36dff"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Week_1_BDA_PySpark.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}